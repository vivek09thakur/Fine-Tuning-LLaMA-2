{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fine-Tuning LLaMA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install accelerate peft bitsandbytes transformers trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Getting Started**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuned_LlaMA:\n",
    "    \n",
    "    def __init__(self,base_model,dataset,new_model):\n",
    "        self.base_model = base_model\n",
    "        self.dataset = dataset\n",
    "        self.new_model = new_model\n",
    "        self.dataset = load_dataset(dataset)\n",
    "        \n",
    "        # 4-bit quantization config\n",
    "        self.compute_dtype = getattr(torch, \"float16\")\n",
    "        self.quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=self.compute_dtype,\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.base_model,\n",
    "            quantization_config=self.quant_config,\n",
    "            device_map={\"\": 0}\n",
    "        )\n",
    "        self.model.config.use_cache = False\n",
    "        self.model.config.pretraining_tp = 1\n",
    "        \n",
    "    def load_tokenizer(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model,trust_remote_code=True)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        \n",
    "        self.peft_params = LoraConfig(\n",
    "                    lora_alpha=16,\n",
    "                    lora_dropout=0.1,\n",
    "                    r=64,\n",
    "                    bias=\"none\",\n",
    "                    task_type=\"CAUSAL_LM\",\n",
    "                )\n",
    "    \n",
    "    def model_trainer(self):\n",
    "        self.training_params = TrainingArguments(\n",
    "                        # The output directory is where the model predictions and checkpoints will be stored.\n",
    "                        output_dir=\"./results\", \n",
    "                        num_train_epochs=1, # One training epoch.\n",
    "                        per_device_train_batch_size=4,\n",
    "                        gradient_accumulation_steps=1,\n",
    "                        optim=\"paged_adamw_32bit\",\n",
    "                        save_steps=25,\n",
    "                        logging_steps=25,\n",
    "                        learning_rate=2e-4,\n",
    "                        weight_decay=0.001,\n",
    "                        fp16=False,\n",
    "                        bf16=False,\n",
    "                        max_grad_norm=0.3,\n",
    "                        max_steps=-1,\n",
    "                        warmup_ratio=0.03,\n",
    "                        group_by_length=True,\n",
    "                        lr_scheduler_type=\"constant\",\n",
    "                        report_to=\"tensorboard\"\n",
    "                )\n",
    "        self.trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            train_dataset=self.dataset,\n",
    "            peft_config=self.peft_params,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=None,\n",
    "            tokenizer=self.tokenizer,\n",
    "            args=self.training_params,\n",
    "            packing=False,\n",
    "        )\n",
    "        \n",
    "        self.trainer.model.save_pretrained(self.new_model)\n",
    "        self.trainer.tokenizer.save_pretrained(self.new_model)\n",
    "        \n",
    "    def create_pipeline(self):\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=100,\n",
    "        )\n",
    "        \n",
    "    def inference(self,prompt):\n",
    "        self.results = self.pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TESTING THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "GUANANCO_DATASET = \"mlabonne/guanaco-llama2-1k\"\n",
    "NEW_MODEL = \"llama2-7b-vivek\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = FineTuned_LlaMA(BASE_MODEL,GUANANCO_DATASET,NEW_MODEL)\n",
    "fine_tuned_model.load_model()\n",
    "fine_tuned_model.load_tokenizer()\n",
    "fine_tuned_model.model_trainer()\n",
    "fine_tuned_model.create_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        prompt = input(\"\\n<You>: \")\n",
    "        if prompt == \"exit\":\n",
    "            break\n",
    "        else:\n",
    "            print(f\"<LlaMA> : {fine_tuned_model.inference(prompt)}\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
